# -*- coding: utf-8 -*-
"""Job clustering project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mj5wAly1fjbYJc6IyrUqO4X3hbpxULlg
"""

pip install requests beautifulsoup4 pandas scikit-learn joblib

import requests
from bs4 import BeautifulSoup
import pandas as pd

base_url = "https://www.karkidi.com/Find-Jobs/1/data-scientist/all"
headers = {"User-Agent": "Mozilla/5.0"}

def scrape_jobs(pages=3):
    jobs = []
    for page in range(1, pages + 1):
        url = f"{base_url}/{page}"
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, "html.parser")

        job_divs = soup.find_all("div", class_="Data Scientist")  # Adjust this as per inspection
        for div in job_divs:
            try:
                title = div.find("h3").text.strip()
                link = div.find("a")["href"]
                company = div.find("span", class_="company").text.strip()
                skills = div.find("div", class_="skills").text.strip()

                jobs.append({
                    "title": title,
                    "company": company,
                    "skills": skills,
                    "url": "https://www.karkidi.com" + link
                })
            except:
                continue
    return pd.DataFrame(jobs)

df = scrape_jobs()
df.to_csv("job_listings.csv", index=False)
print("Saved job listings to job_listings.csv")

from bs4 import BeautifulSoup

html_content = """<div id="getJobs_replace">
    <div class="loaderClass" id="loaderClass">
    <div class="ads-details bg-white border2">
    <div class="cmp-details">
        <div class="cmp-logo"> <a href="#"> <img src="https://www.karkidi.com/upload-nct/company-logo/th3_surestart_1bd25.png" alt="SureStart"> </a> </div>
        <div class="cmp-info">
            <a href="https://www.karkidi.com/job-details/89562-data-in-action-mentor-summer-2025-job" target="_blank">
                <h4>Data In Action Mentor (Summer 2025)</h4>
            </a>
            <a href="https://www.karkidi.com/Employer-Profile/9367-surestart-jobs" target="_blank">SureStart</a>
            <p><i class="fa fa fa-map-marker"></i>New York, NY, USA</p>
            <p class="emp-exp">0-2 year</p>
        </div>
    </div>
    <div class="" style="display: none">0_0_0_0</div>
    <div class="hour-details text-right">
        <span class="label label-primary"></span>
        <span class="label label-warning fulltime">Full Time</span>
        <p>24 May 2025</p>
    </div>
    <div class="apply-now-main">
        <div class="apply-now">
            <a class="apply-now-btn Apply-job removeApplyBtn_89562" href="javascript:void(0);" data-id="89562"><i class="fa fa-paper-plane" aria-hidden="true"></i> Apply Now</a>
        </div>
        <div class="msg-cell">
            <span class="left-content">Summary</span>
            <p class="text-black"></p>
            <p>SureStart is looking for undergraduate mentors with experience in Data Science, and a passion for community empowerment, to help in our Data in Action summer program! It will also help you develop...</p>
        </div>
        <div class="msg-cell">
            <span class="left-content">Key Skills</span>
            <p class="text-greey">Data Analytics,Data science techniques,Effective communication skills </p>
        </div>
    </div>
    <div class="clearfix"></div>
</div>
<!-- Repeat similar blocks for other jobs -->
</div>
</div>"""

soup = BeautifulSoup(html_content, "html.parser")

jobs = []

# Select all job blocks
job_blocks = soup.find_all("div", class_="ads-details")

for job in job_blocks:
    company_logo = job.find("div", class_="cmp-logo").find("img")["src"] if job.find("div", class_="cmp-logo") else None
    company_name = job.find("div", class_="cmp-info").find_all("a")[1].text.strip() if job.find("div", class_="cmp-info") else None
    job_title = job.find("div", class_="cmp-info").find("h4").text.strip() if job.find("div", class_="cmp-info") else None
    job_link = job.find("div", class_="cmp-info").find("a")["href"] if job.find("div", class_="cmp-info") else None
    location = job.find("div", class_="cmp-info").find("p").text.strip() if job.find("div", class_="cmp-info") else None
    experience = job.find("p", class_="emp-exp").text.strip() if job.find("p", class_="emp-exp") else None
    job_type = job.find("span", class_="label-warning").text.strip() if job.find("span", class_="label-warning") else None
    date_posted = job.find("div", class_="hour-details").find_all("p")[0].text.strip() if job.find("div", class_="hour-details") else None
    summary = job.find("div", class_="apply-now-main").find_all("div", class_="msg-cell")[0].find_all("p")[1].text.strip()
    key_skills = job.find("div", class_="apply-now-main").find_all("div", class_="msg-cell")[1].find("p").text.strip()

    jobs.append({
        "company_logo": company_logo,
        "company_name": company_name,
        "job_title": job_title,
        "job_link": job_link,
        "location": location,
        "experience": experience,
        "job_type": job_type,
        "date_posted": date_posted,
        "summary": summary,
        "key_skills": key_skills
    })

# Print extracted jobs
for idx, j in enumerate(jobs, 1):
    print(f"Job {idx}:")
    for k, v in j.items():
        print(f"{k}: {v}")
    print("-" * 40)

"""Scraping"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_jobs(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    jobs = []
    for job_div in soup.find_all('div', class_='job-post'):
        title = job_div.find('h2').text.strip()
        company = job_div.find('span', class_='company').text.strip()
        skills = job_div.find('p', class_='skills').text.strip()
        jobs.append({'title': title, 'company': company, 'skills': skills})

    return pd.DataFrame(jobs)

# Example usage
url = 'https://www.karkidi.com/jobs'
jobs_df = scrape_jobs(url)
jobs_df.to_csv('jobs.csv', index=False)

"""Preprocessing

"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

def preprocess_data(file_path):
    df = pd.read_csv(file_path)

    # Ensure 'skills' column exists and is filled
    if 'skills' not in df.columns or df['skills'].isnull().all():
        raise ValueError("The 'skills' column is missing or empty.")

    # Lowercase and remove punctuation
    df['skills'] = df['skills'].str.lower().str.replace(r'[^\w\s]', '', regex=True)

    # TF-IDF Vectorization
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(df['skills'])

    return df, X, vectorizer

# Example usage
df, X, vectorizer = preprocess_data('sample_jobs.csv')

"""Clustering"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Step 1: Choose number of clusters using Elbow Method (optional)
def plot_elbow_method(X, max_k=10):
    sse = []
    for k in range(1, max_k + 1):
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        sse.append(kmeans.inertia_)
    plt.plot(range(1, max_k + 1), sse, marker='o')
    plt.xlabel('Number of Clusters (k)')
    plt.ylabel('SSE (Inertia)')
    plt.title('Elbow Method to Determine Optimal k')
    plt.show()

# Uncomment to visualize elbow method
# plot_elbow_method(X)

# Step 2: Apply KMeans clustering
optimal_k = 5  # Choose based on elbow method or trial
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans.fit(X)

# Step 3: Assign cluster labels to your dataframe
df['cluster'] = kmeans.labels_

# View the clustered data
print(df[['job_title', 'skills', 'cluster']].head())

"""Model Saving"""

import joblib

# Save the clustering model
joblib.dump(kmeans, 'job_clusters_model.pkl')

# Save the TF-IDF vectorizer
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

"""Alerting system"""

def alert_new_jobs(df, skills_keywords, vectorizer, model):
    """
    Checks if any new job postings match the given skills_keywords.
    """
    from sklearn.metrics.pairwise import cosine_similarity

    # Convert keywords to TF-IDF
    input_vec = vectorizer.transform([skills_keywords.lower()])

    # Compare with existing job skill vectors
    similarities = cosine_similarity(input_vec, vectorizer.transform(df['skills']))
    df['similarity'] = similarities[0]

    # Return top matches above a threshold
    matches = df[df['similarity'] > 0.3].sort_values(by='similarity', ascending=False)
    return matches[['job_title', 'skills', 'similarity']]

# Example usage
alert_jobs = alert_new_jobs(df, "python machine learning sql", vectorizer, kmeans)
print(alert_jobs.head())

